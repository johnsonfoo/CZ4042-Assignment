{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpbmSdmWpW8o"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VacDzrT5mo5"
   },
   "outputs": [],
   "source": [
    "# Setting the seed here is sufficient. \n",
    "# If you don't plan to use these starter code, make sure you add this cell.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NujeG24-5R6c"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZwgqhzepW8u"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "5cn1IOWBmgB0",
    "outputId": "fc27518d-4d57-4666-acb1-c1b204ce54eb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dceWqrra5R4d"
   },
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('resale_price')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(\n",
    "        output_mode='binary')  # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEChfxIqpW8w"
   },
   "source": [
    "# Question 1\n",
    "\n",
    "Real world datasets often contain a mix of numeric and categorical features – this dataset is one such example. Modelling such a mix of feature types with neural networks requires some modifications to the input layer. This tutorial from the Keras documentation guides you through the process of using the Functional API to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RGmmI5ApW80"
   },
   "outputs": [],
   "source": [
    "class Question1():\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, epochs=100):\n",
    "        self.epochs = epochs\n",
    "        self.seed = 42\n",
    "        self.filepath = 'p2q1f.ckpt'\n",
    "        self.history = None\n",
    "\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def prepare_model(self):\n",
    "        # integer categorical feature input\n",
    "        month = keras.Input(shape=(1,), name='month', dtype='int64')\n",
    "\n",
    "        # string categorical features input\n",
    "        storey_range = keras.Input(shape=(1,), name='storey_range', dtype='string')\n",
    "        flat_model_type = keras.Input(shape=(1,), name='flat_model_type', dtype='string')\n",
    "\n",
    "        # numerical features input\n",
    "        floor_area_sqm = keras.Input(shape=(1,), name='floor_area_sqm')\n",
    "        remaining_lease_years = keras.Input(shape=(1,), name='remaining_lease_years')\n",
    "        degree_centrality = keras.Input(shape=(1,), name='degree_centrality')\n",
    "        eigenvector_centrality = keras.Input(shape=(1,), name='eigenvector_centrality')\n",
    "        dist_to_nearest_stn = keras.Input(shape=(1,), name='dist_to_nearest_stn')\n",
    "        dist_to_dhoby = keras.Input(shape=(1,), name='dist_to_dhoby')\n",
    "\n",
    "        self.all_inputs = [month,\n",
    "                           storey_range,\n",
    "                           flat_model_type,\n",
    "                           floor_area_sqm,\n",
    "                           remaining_lease_years,\n",
    "                           degree_centrality,\n",
    "                           eigenvector_centrality,\n",
    "                           dist_to_nearest_stn,\n",
    "                           dist_to_dhoby]\n",
    "\n",
    "        # integer categorical feature encoded\n",
    "        month_encoded = encode_categorical_feature(month, 'month', self.train_ds, False)\n",
    "\n",
    "        # string categorical features encoded\n",
    "        storey_range_encoded = encode_categorical_feature(storey_range, 'storey_range', self.train_ds, True)\n",
    "        flat_model_type_encoded = encode_categorical_feature(flat_model_type, 'flat_model_type', self.train_ds, True)\n",
    "\n",
    "        # numerical features encoded\n",
    "        floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, 'floor_area_sqm', self.train_ds)\n",
    "        remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, 'remaining_lease_years',\n",
    "                                                                 self.train_ds)\n",
    "        degree_centrality_encoded = encode_numerical_feature(degree_centrality, 'degree_centrality', self.train_ds)\n",
    "        eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, 'eigenvector_centrality',\n",
    "                                                                  self.train_ds)\n",
    "        dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, 'dist_to_nearest_stn',\n",
    "                                                               self.train_ds)\n",
    "        dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, 'dist_to_dhoby', self.train_ds)\n",
    "\n",
    "        self.all_features = tf.keras.layers.concatenate([month_encoded,\n",
    "                                                         storey_range_encoded,\n",
    "                                                         flat_model_type_encoded,\n",
    "                                                         floor_area_sqm_encoded,\n",
    "                                                         remaining_lease_years_encoded,\n",
    "                                                         degree_centrality_encoded,\n",
    "                                                         eigenvector_centrality_encoded,\n",
    "                                                         dist_to_nearest_stn_encoded,\n",
    "                                                         dist_to_dhoby_encoded])\n",
    "\n",
    "    @staticmethod\n",
    "    def r_square(y_true, y_pred):\n",
    "        SS_res = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))\n",
    "        SS_tot = tf.keras.backend.sum(tf.keras.backend.square(y_true - tf.keras.backend.mean(y_true)))\n",
    "        return (1 - SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def create_model(self):\n",
    "        # create the model\n",
    "        x = tf.keras.layers.Dense(units=10, activation='relu')(self.all_features)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='linear')(x)\n",
    "        self.model = tf.keras.Model(inputs=self.all_inputs, outputs=output)\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "                           loss=tf.keras.losses.MeanSquaredError(),\n",
    "                           metrics=[self.r_square])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self):\n",
    "        # train the model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.filepath,\n",
    "                                                                       monitor='val_loss',\n",
    "                                                                       verbose=1,\n",
    "                                                                       save_best_only=True,\n",
    "                                                                       save_weights_only=True,\n",
    "                                                                       mode='min')\n",
    "        self.history = self.model.fit(x=train_ds,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=0,\n",
    "                                      callbacks=[model_checkpoint_callback],\n",
    "                                      validation_data=self.val_ds)\n",
    "\n",
    "    def plot_model(self, variable='loss', epoch_start=1, epoch_end=None):\n",
    "        # plot learning curves\n",
    "        train_epochs = self.history.history[variable][epoch_start - 1:epoch_end]\n",
    "        val_epochs = self.history.history['val_{}'.format(variable)][epoch_start - 1:epoch_end]\n",
    "        number_epochs = len(self.history.history[variable][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs,\n",
    "                 label='Model training {}'.format(variable))\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rmse_model(self, epoch_start=1, epoch_end=None):\n",
    "        # plot rmse curves\n",
    "        train_epochs = list(map(lambda x: math.sqrt(x), self.history.history['loss'][epoch_start - 1:epoch_end]))\n",
    "        val_epochs = list(map(lambda x: math.sqrt(x), self.history.history['val_loss'][epoch_start - 1:epoch_end]))\n",
    "        number_epochs = len(self.history.history['loss'][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs, label='Model training rmse')\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing rmse')\n",
    "        plt.title('Model rmse')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def get_lowest_test_error_model(self):\n",
    "        # get lowest test error epoch and R2 value\n",
    "        lowest_test_error, lowest_test_error_epoch = min(\n",
    "            (value, index + 1) for (index, value) in enumerate(self.history.history['val_loss']))\n",
    "        print('Lowest test error: {}'.format(lowest_test_error))\n",
    "        print('Lowest test error epoch: {}'.format(lowest_test_error_epoch))\n",
    "        print('Lowest test error epoch\\'s R2 value: {}'.format(\n",
    "            self.history.history['val_r_square'][lowest_test_error_epoch - 1]))\n",
    "\n",
    "    def restore_best_model_and_predict(self):\n",
    "        # restore weights to model from lowest test error epoch\n",
    "        print('Before restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('After restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.load_weights(filepath=self.filepath)\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('')\n",
    "        target_values = list(self.val_ds.take(1).as_numpy_iterator())[0][1]\n",
    "        predicted_values = self.model.predict(x=self.val_ds, steps=1)\n",
    "\n",
    "        # plot targets and predictions\n",
    "        plt.figure(1, figsize=(32, 4))\n",
    "        plt.plot(target_values, 'b^', label='Model target values')\n",
    "        plt.plot(predicted_values, 'ro', label='Model predicted values')\n",
    "        plt.title('Best model target values and predicted values')\n",
    "        plt.ylabel('resale price')\n",
    "        plt.xlabel('ith test sample from batch 1')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53TCwrchpW8x"
   },
   "source": [
    "a) Divide the dataset (‘HDB_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data (with the remaining data from year 2021 used as test data). Why is this done instead of random train/test splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TavIPVRM5R2Y"
   },
   "outputs": [],
   "source": [
    "#Split data\n",
    "train_dataframe = df[df['year'] <= 2020].reset_index(drop=True)\n",
    "val_dataframe = df[df['year'] == 2021].reset_index(drop=True)\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "train_ds = train_ds.batch(128)\n",
    "val_ds = val_ds.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ2y_RuNpW8z"
   },
   "source": [
    "b) Following this tutorial, design a 2-layer feedforward neural network consisting of an input layer, a hidden layer (10 neurons, ReLU as activation function), and a linear output layer. One-hot encoding should be applied to categorical features and numeric features are standardised. After encoding / standardisation, the input features should be concatenated.\n",
    "\n",
    "The input layer should use these features:\n",
    "- Numeric features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "\n",
    "- Categorical features: month, flat_model_type, storey_range\n",
    "\n",
    "Your architecture should resemble the figure shown on the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "OUUX9J_UFJ7e",
    "outputId": "30e5ba87-82da-40b1-ed8e-aad156338b54"
   },
   "outputs": [],
   "source": [
    "question1 = Question1(train_ds, val_ds)\n",
    "question1.prepare_model()\n",
    "question1.create_model()\n",
    "keras.utils.plot_model(model=question1.model, to_file='p2q1b.png', show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqNiY84XRh_P"
   },
   "source": [
    "c) On the training data, train the model for 100 epochs using mini-batch gradient descent with batch size = 128, Use ‘adam’ optimiser with a learning rate of alpha = 0.05 and mean square error as cost function. (Tip: Use smaller epochs while you’re still debugging. On Google Colaboratory, 100 epochs take around 10 minutes even without GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOVEZQBHFaFE",
    "outputId": "bb2af2f1-1d8e-419b-f86e-2963537b0bdb"
   },
   "outputs": [],
   "source": [
    "question1.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dUvUI1gRzwf"
   },
   "source": [
    "d) Plot the train and test root mean square errors (RMSE) against epochs (Tip: skip the first few epochs, else the plot gets dominated by them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "xilUKTQXemGz",
    "outputId": "620e0a93-2b4d-4478-81ea-b867e1f98eb0"
   },
   "outputs": [],
   "source": [
    "question1.plot_rmse_model(epoch_start=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVURT2WNR2ks"
   },
   "source": [
    "e) State the epoch with the lowest test error. State the test R2 value at that epoch. (Hint: Check the output returned by model.fit(). Use a custom metric for computing R2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEg4YBKIZ12a",
    "outputId": "e448d2ea-5b96-4a61-aaad-bf468ec3c494"
   },
   "outputs": [],
   "source": [
    "question1.get_lowest_test_error_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaZdlKgXR4ou"
   },
   "source": [
    "f) Using the model from that best epoch, plot the predicted values and target values for a batch of 128 test samples. (Hint: Use a callback to restore the best model weights. Find out how to retrieve a batch from tf.BatchDataset. A scatter plot will suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "N-pGovc8iA_9",
    "outputId": "8d998660-d125-482e-f695-55373a535b4d"
   },
   "outputs": [],
   "source": [
    "question1.restore_best_model_and_predict()  # test loss and test r_square after restore best model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GHgcfbq4LMZ"
   },
   "source": [
    "# Question 2\n",
    "\n",
    "Instead of using one-hot encoding, an alternative approach entails the use of embeddings to encode categorical variables. Such an approach utilises the ability of neural networks to learn richer representations of the data – an edge it has over traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aulaNDYj4JNl"
   },
   "outputs": [],
   "source": [
    "class Question2():\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, epochs=100):\n",
    "        self.epochs = epochs\n",
    "        self.seed = 42\n",
    "        self.filepath = 'p2q2b.ckpt'\n",
    "        self.history = None\n",
    "\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def modified_encode_categorical_feature(feature, name, dataset, is_string):\n",
    "        lookup_class = StringLookup if is_string else IntegerLookup\n",
    "        lookup = lookup_class(\n",
    "            output_mode='int')\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "        lookup.adapt(feature_ds)\n",
    "        encoded_feature = lookup(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "    def prepare_model(self):\n",
    "        # integer categorical feature input\n",
    "        month = keras.Input(shape=(1,), name='month', dtype='int64')\n",
    "\n",
    "        # string categorical features input\n",
    "        storey_range = keras.Input(shape=(1,), name='storey_range', dtype='string')\n",
    "        flat_model_type = keras.Input(shape=(1,), name='flat_model_type', dtype='string')\n",
    "\n",
    "        # numerical features input\n",
    "        floor_area_sqm = keras.Input(shape=(1,), name='floor_area_sqm')\n",
    "        remaining_lease_years = keras.Input(shape=(1,), name='remaining_lease_years')\n",
    "        degree_centrality = keras.Input(shape=(1,), name='degree_centrality')\n",
    "        eigenvector_centrality = keras.Input(shape=(1,), name='eigenvector_centrality')\n",
    "        dist_to_nearest_stn = keras.Input(shape=(1,), name='dist_to_nearest_stn')\n",
    "        dist_to_dhoby = keras.Input(shape=(1,), name='dist_to_dhoby')\n",
    "\n",
    "        self.all_inputs = [month,\n",
    "                           storey_range,\n",
    "                           flat_model_type,\n",
    "                           floor_area_sqm,\n",
    "                           remaining_lease_years,\n",
    "                           degree_centrality,\n",
    "                           eigenvector_centrality,\n",
    "                           dist_to_nearest_stn,\n",
    "                           dist_to_dhoby]\n",
    "\n",
    "        # integer categorical feature modified encoded\n",
    "        month_modified_encoded = self.modified_encode_categorical_feature(month, 'month', self.train_ds, False)\n",
    "\n",
    "        # string categorical features modified encoded\n",
    "        storey_range_modified_encoded = self.modified_encode_categorical_feature(storey_range, 'storey_range',\n",
    "                                                                                 self.train_ds, True)\n",
    "        flat_model_type_modified_encoded = self.modified_encode_categorical_feature(flat_model_type, 'flat_model_type',\n",
    "                                                                                    self.train_ds, True)\n",
    "\n",
    "        # numerical features encoded\n",
    "        floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, 'floor_area_sqm', self.train_ds)\n",
    "        remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, 'remaining_lease_years',\n",
    "                                                                 self.train_ds)\n",
    "        degree_centrality_encoded = encode_numerical_feature(degree_centrality, 'degree_centrality', self.train_ds)\n",
    "        eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, 'eigenvector_centrality',\n",
    "                                                                  self.train_ds)\n",
    "        dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, 'dist_to_nearest_stn',\n",
    "                                                               self.train_ds)\n",
    "        dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, 'dist_to_dhoby', self.train_ds)\n",
    "\n",
    "        # integer categorical feature embedded and flattened\n",
    "        month_embedding = tf.keras.layers.Embedding(input_dim=13, output_dim=(13 - 1) // 2)(month_modified_encoded)\n",
    "        month_embedding_flattened = tf.keras.layers.Flatten()(month_embedding)\n",
    "\n",
    "        # string categorical features embedded and flattened\n",
    "        storey_range_embedding = tf.keras.layers.Embedding(input_dim=18, output_dim=(18 - 1) // 2)(\n",
    "            storey_range_modified_encoded)\n",
    "        storey_range_embedding_flattened = tf.keras.layers.Flatten()(storey_range_embedding)\n",
    "        flat_model_type_embedding = tf.keras.layers.Embedding(input_dim=44, output_dim=(44 - 1) // 2)(\n",
    "            flat_model_type_modified_encoded)\n",
    "        flat_model_type_embedding_flattened = tf.keras.layers.Flatten()(flat_model_type_embedding)\n",
    "\n",
    "        self.all_features = tf.keras.layers.concatenate([month_embedding_flattened,\n",
    "                                                         storey_range_embedding_flattened,\n",
    "                                                         flat_model_type_embedding_flattened,\n",
    "                                                         floor_area_sqm_encoded,\n",
    "                                                         remaining_lease_years_encoded,\n",
    "                                                         degree_centrality_encoded,\n",
    "                                                         eigenvector_centrality_encoded,\n",
    "                                                         dist_to_nearest_stn_encoded,\n",
    "                                                         dist_to_dhoby_encoded])\n",
    "\n",
    "    @staticmethod\n",
    "    def r_square(y_true, y_pred):\n",
    "        SS_res = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))\n",
    "        SS_tot = tf.keras.backend.sum(tf.keras.backend.square(y_true - tf.keras.backend.mean(y_true)))\n",
    "        return (1 - SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def create_model(self):\n",
    "        # create the model\n",
    "        x = tf.keras.layers.Dense(units=10, activation='relu')(self.all_features)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='linear')(x)\n",
    "        self.model = tf.keras.Model(inputs=self.all_inputs, outputs=output)\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "                           loss=tf.keras.losses.MeanSquaredError(),\n",
    "                           metrics=[self.r_square])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self):\n",
    "        # train the model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.filepath,\n",
    "                                                                       monitor='val_loss',\n",
    "                                                                       verbose=1,\n",
    "                                                                       save_best_only=True,\n",
    "                                                                       save_weights_only=True,\n",
    "                                                                       mode='min')\n",
    "        self.history = self.model.fit(x=train_ds,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=0,\n",
    "                                      callbacks=[model_checkpoint_callback],\n",
    "                                      validation_data=self.val_ds)\n",
    "\n",
    "    def plot_model(self, variable='loss', epoch_start=1, epoch_end=None):\n",
    "        # plot learning curves\n",
    "        train_epochs = self.history.history[variable][epoch_start - 1:epoch_end]\n",
    "        val_epochs = self.history.history['val_{}'.format(variable)][epoch_start - 1:epoch_end]\n",
    "        number_epochs = len(self.history.history[variable][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs,\n",
    "                 label='Model training {}'.format(variable))\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rmse_model(self, epoch_start=1, epoch_end=None):\n",
    "        # plot rmse curves\n",
    "        train_epochs = list(map(lambda x: math.sqrt(x), self.history.history['loss'][epoch_start - 1:epoch_end]))\n",
    "        val_epochs = list(map(lambda x: math.sqrt(x), self.history.history['val_loss'][epoch_start - 1:epoch_end]))\n",
    "        number_epochs = len(self.history.history['loss'][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs, label='Model training rmse')\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing rmse')\n",
    "        plt.title('Model rmse')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def get_lowest_test_error_model(self):\n",
    "        # get lowest test error epoch and R2 value\n",
    "        lowest_test_error, lowest_test_error_epoch = min(\n",
    "            (value, index + 1) for (index, value) in enumerate(self.history.history['val_loss']))\n",
    "        print('Lowest test error: {}'.format(lowest_test_error))\n",
    "        print('Lowest test error epoch: {}'.format(lowest_test_error_epoch))\n",
    "        print('Lowest test error epoch\\'s R2 value: {}'.format(\n",
    "            self.history.history['val_r_square'][lowest_test_error_epoch - 1]))\n",
    "\n",
    "    def restore_best_model_and_predict(self):\n",
    "        # restore weights to model from lowest test error epoch\n",
    "        print('Before restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('After restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.load_weights(filepath=self.filepath)\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('')\n",
    "        target_values = list(self.val_ds.take(1).as_numpy_iterator())[0][1]\n",
    "        predicted_values = self.model.predict(x=self.val_ds, steps=1)\n",
    "\n",
    "        # plot targets and predictions\n",
    "        plt.figure(1, figsize=(32, 4))\n",
    "        plt.plot(target_values, 'b^', label='Model target values')\n",
    "        plt.plot(predicted_values, 'ro', label='Model predicted values')\n",
    "        plt.title('Best model target values and predicted values')\n",
    "        plt.ylabel('resale price')\n",
    "        plt.xlabel('ith test sample from batch 1')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L748oidh8URO"
   },
   "source": [
    "a) Add an Embedding layer with output_dim = floor(num_categories/2) after the one-hot embeddings for categorical variables. (Hint: Use the tf.keras.layers.Embedding() later. Read the documentation carefully to ensure that you define the correct function parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "-ScGi3Zg8gAx",
    "outputId": "429ac177-e722-4dfe-c203-c5678678885b"
   },
   "outputs": [],
   "source": [
    "question2 = Question2(train_ds, val_ds)\n",
    "question2.prepare_model()\n",
    "question2.create_model()\n",
    "keras.utils.plot_model(model=question2.model, to_file='p2q2a.png', show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGy-qkaSyQbm"
   },
   "source": [
    "b) The Embedding layer produces a 2D output (3D, including batch), which cannot be concatenated with the other features. Look through the Keras layers API to determine which layer to add in, such that all the features can be concatenated. Train the model using the same configuration as Q1. (Tip: A full run takes ~15 mins, so reduce epochs when debugging your code but remember to switch it back to 100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCfN4BJ8y942",
    "outputId": "604cda44-e62a-4b83-d009-addf673c6b7b"
   },
   "outputs": [],
   "source": [
    "question2.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwBkunFqqlwQ"
   },
   "source": [
    "c) Compare the current model performances in terms of both test RMSE and test R2 with the model from Q1 (at their own best epochs) and suggest a possible reason for the difference in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "p0pnV2KlpJxS",
    "outputId": "5f2aee2b-939e-4961-8e54-90b2184c256f"
   },
   "outputs": [],
   "source": [
    "question2.plot_rmse_model(epoch_start=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKx2wJLPpLso",
    "outputId": "16051bf6-5525-476f-a54d-2101c45bbf10"
   },
   "outputs": [],
   "source": [
    "question2.get_lowest_test_error_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "0WGU24P_pOH4",
    "outputId": "ffa0b224-cec9-44b6-c8c8-b1590eebfc32"
   },
   "outputs": [],
   "source": [
    "question2.restore_best_model_and_predict()  # test loss and test r_square after restore best model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Recursive feature elimination (RFE) is a feature selection method that removes unnecessary features from the inputs. It can also shed some insights on how much each feature contributes to the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question3():\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, epochs=100):\n",
    "        self.epochs = epochs\n",
    "        self.seed = 42\n",
    "        self.filepath = 'p2q3a.ckpt'\n",
    "        self.history = None\n",
    "\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def modified_encode_categorical_feature(feature, name, dataset, is_string):\n",
    "        lookup_class = StringLookup if is_string else IntegerLookup\n",
    "        lookup = lookup_class(\n",
    "            output_mode='int')\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "        lookup.adapt(feature_ds)\n",
    "        encoded_feature = lookup(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "    def prepare_model(self):\n",
    "        # integer categorical feature input\n",
    "        month = keras.Input(shape=(1,), name='month', dtype='int64')\n",
    "\n",
    "        # string categorical features input\n",
    "        storey_range = keras.Input(shape=(1,), name='storey_range', dtype='string')\n",
    "        flat_model_type = keras.Input(shape=(1,), name='flat_model_type', dtype='string')\n",
    "\n",
    "        # numerical features input\n",
    "        floor_area_sqm = keras.Input(shape=(1,), name='floor_area_sqm')\n",
    "        remaining_lease_years = keras.Input(shape=(1,), name='remaining_lease_years')\n",
    "        degree_centrality = keras.Input(shape=(1,), name='degree_centrality')\n",
    "        eigenvector_centrality = keras.Input(shape=(1,), name='eigenvector_centrality')\n",
    "        dist_to_nearest_stn = keras.Input(shape=(1,), name='dist_to_nearest_stn')\n",
    "        dist_to_dhoby = keras.Input(shape=(1,), name='dist_to_dhoby')\n",
    "\n",
    "        self.all_inputs = [month,\n",
    "                           storey_range,\n",
    "                           flat_model_type,\n",
    "                           floor_area_sqm,\n",
    "                           remaining_lease_years,\n",
    "                           degree_centrality,\n",
    "                           eigenvector_centrality,\n",
    "                           dist_to_nearest_stn,\n",
    "                           dist_to_dhoby]\n",
    "\n",
    "        # integer categorical feature modified encoded\n",
    "        month_modified_encoded = self.modified_encode_categorical_feature(month, 'month', self.train_ds, False)\n",
    "\n",
    "        # string categorical features modified encoded\n",
    "        storey_range_modified_encoded = self.modified_encode_categorical_feature(storey_range, 'storey_range',\n",
    "                                                                                 self.train_ds, True)\n",
    "        flat_model_type_modified_encoded = self.modified_encode_categorical_feature(flat_model_type, 'flat_model_type',\n",
    "                                                                                    self.train_ds, True)\n",
    "\n",
    "        # numerical features encoded\n",
    "        floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, 'floor_area_sqm', self.train_ds)\n",
    "        remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, 'remaining_lease_years',\n",
    "                                                                 self.train_ds)\n",
    "        degree_centrality_encoded = encode_numerical_feature(degree_centrality, 'degree_centrality', self.train_ds)\n",
    "        eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, 'eigenvector_centrality',\n",
    "                                                                  self.train_ds)\n",
    "        dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, 'dist_to_nearest_stn',\n",
    "                                                               self.train_ds)\n",
    "        dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, 'dist_to_dhoby', self.train_ds)\n",
    "\n",
    "        # integer categorical feature embedded and flattened\n",
    "        month_embedding = tf.keras.layers.Embedding(input_dim=13, output_dim=(13 - 1) // 2)(month_modified_encoded)\n",
    "        month_embedding_flattened = tf.keras.layers.Flatten()(month_embedding)\n",
    "\n",
    "        # string categorical features embedded and flattened\n",
    "        storey_range_embedding = tf.keras.layers.Embedding(input_dim=18, output_dim=(18 - 1) // 2)(\n",
    "            storey_range_modified_encoded)\n",
    "        storey_range_embedding_flattened = tf.keras.layers.Flatten()(storey_range_embedding)\n",
    "        flat_model_type_embedding = tf.keras.layers.Embedding(input_dim=44, output_dim=(44 - 1) // 2)(\n",
    "            flat_model_type_modified_encoded)\n",
    "        flat_model_type_embedding_flattened = tf.keras.layers.Flatten()(flat_model_type_embedding)\n",
    "\n",
    "        self.all_features = tf.keras.layers.concatenate([month_embedding_flattened,\n",
    "                                                         storey_range_embedding_flattened,\n",
    "                                                         flat_model_type_embedding_flattened,\n",
    "                                                         floor_area_sqm_encoded,\n",
    "                                                         remaining_lease_years_encoded,\n",
    "                                                         degree_centrality_encoded,\n",
    "                                                         eigenvector_centrality_encoded,\n",
    "                                                         dist_to_nearest_stn_encoded,\n",
    "                                                         dist_to_dhoby_encoded])\n",
    "\n",
    "    @staticmethod\n",
    "    def r_square(y_true, y_pred):\n",
    "        SS_res = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))\n",
    "        SS_tot = tf.keras.backend.sum(tf.keras.backend.square(y_true - tf.keras.backend.mean(y_true)))\n",
    "        return (1 - SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def create_model(self):\n",
    "        # create the model\n",
    "        x = tf.keras.layers.Dense(units=10, activation='relu')(self.all_features)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='linear')(x)\n",
    "        self.model = tf.keras.Model(inputs=self.all_inputs, outputs=output)\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "                           loss=tf.keras.losses.MeanSquaredError(),\n",
    "                           metrics=[self.r_square])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self):\n",
    "        # train the model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.filepath,\n",
    "                                                                       monitor='val_loss',\n",
    "                                                                       verbose=1,\n",
    "                                                                       save_best_only=True,\n",
    "                                                                       save_weights_only=True,\n",
    "                                                                       mode='min')\n",
    "        self.history = self.model.fit(x=train_ds,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=0,\n",
    "                                      callbacks=[EarlyStopping(monitor='val_loss', patience=10),\n",
    "                                                 model_checkpoint_callback],\n",
    "                                      validation_data=self.val_ds)\n",
    "\n",
    "    def plot_model(self, variable='loss', epoch_start=1, epoch_end=None):\n",
    "        # plot learning curves\n",
    "        train_epochs = self.history.history[variable][epoch_start - 1:epoch_end]\n",
    "        val_epochs = self.history.history['val_{}'.format(variable)][epoch_start - 1:epoch_end]\n",
    "        number_epochs = len(self.history.history[variable][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs,\n",
    "                 label='Model training {}'.format(variable))\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rmse_model(self, epoch_start=1, epoch_end=None):\n",
    "        # plot rmse curves\n",
    "        train_epochs = list(map(lambda x: math.sqrt(x), self.history.history['loss'][epoch_start - 1:epoch_end]))\n",
    "        val_epochs = list(map(lambda x: math.sqrt(x), self.history.history['val_loss'][epoch_start - 1:epoch_end]))\n",
    "        number_epochs = len(self.history.history['loss'][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs, label='Model training rmse')\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing rmse')\n",
    "        plt.title('Model rmse')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def get_lowest_test_error_model(self):\n",
    "        # get lowest test error epoch and R2 value\n",
    "        lowest_test_error, lowest_test_error_epoch = min(\n",
    "            (value, index + 1) for (index, value) in enumerate(self.history.history['val_loss']))\n",
    "        print('Lowest test error: {}'.format(lowest_test_error))\n",
    "        print('Lowest test error epoch: {}'.format(lowest_test_error_epoch))\n",
    "        print('Lowest test error epoch\\'s R2 value: {}'.format(\n",
    "            self.history.history['val_r_square'][lowest_test_error_epoch - 1]))\n",
    "\n",
    "    def restore_best_model_and_predict(self):\n",
    "        # restore weights to model from lowest test error epoch\n",
    "        print('Before restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('After restore best model weights, model has test loss and test r_square as follows:')\n",
    "        self.model.load_weights(filepath=self.filepath)\n",
    "        self.model.evaluate(self.val_ds)\n",
    "        print('')\n",
    "        target_values = list(self.val_ds.take(1).as_numpy_iterator())[0][1]\n",
    "        predicted_values = self.model.predict(x=self.val_ds, steps=1)\n",
    "\n",
    "        # plot targets and predictions\n",
    "        plt.figure(1, figsize=(32, 4))\n",
    "        plt.plot(target_values, 'b^', label='Model target values')\n",
    "        plt.plot(predicted_values, 'ro', label='Model predicted values')\n",
    "        plt.title('Best model target values and predicted values')\n",
    "        plt.ylabel('resale price')\n",
    "        plt.xlabel('ith test sample from batch 1')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Continue with the model architecture you have after Q2. Via a callback, introduce early stopping (based on val_loss, with patience of 10 epochs) to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = Question3(train_ds, val_ds)\n",
    "question3.prepare_model()\n",
    "question3.create_model()\n",
    "keras.utils.plot_model(model=question3.model, to_file='p2q3a.png', show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Start by removing one input feature whose removal leads to the minimum drop (or maximum improvement) in performance. Repeat the procedure recursively on the reduced input set until the optimal number of input features is reached. Remember to remove features one at a time. Record the RMSE of each experiment neatly in a table (i.e., without feature 1, without feature 2, etc.). (Hint: Use a binary vector mask to keep track of the features. When you remove a feature, you do not have to repeatedly remove the initialisation of the input layers for each feature. Just choose which to include when you concatenate the features. Make sure to clear the session at every iteration of feature elimination. A full run take ~2hrs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question3RFEHelper():\n",
    "\n",
    "    def __init__(self, train_ds, val_ds):\n",
    "        self.seed = 42\n",
    "\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def modified_encode_categorical_feature(feature, name, dataset, is_string):\n",
    "        lookup_class = StringLookup if is_string else IntegerLookup\n",
    "        lookup = lookup_class(\n",
    "            output_mode='int')\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "        lookup.adapt(feature_ds)\n",
    "        encoded_feature = lookup(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "    def prepare_rfe(self):\n",
    "        # integer categorical feature input\n",
    "        month = keras.Input(shape=(1,), name='month', dtype='int64')\n",
    "\n",
    "        # string categorical features input\n",
    "        storey_range = keras.Input(shape=(1,), name='storey_range', dtype='string')\n",
    "        flat_model_type = keras.Input(shape=(1,), name='flat_model_type', dtype='string')\n",
    "\n",
    "        # numerical features input\n",
    "        floor_area_sqm = keras.Input(shape=(1,), name='floor_area_sqm')\n",
    "        remaining_lease_years = keras.Input(shape=(1,), name='remaining_lease_years')\n",
    "        degree_centrality = keras.Input(shape=(1,), name='degree_centrality')\n",
    "        eigenvector_centrality = keras.Input(shape=(1,), name='eigenvector_centrality')\n",
    "        dist_to_nearest_stn = keras.Input(shape=(1,), name='dist_to_nearest_stn')\n",
    "        dist_to_dhoby = keras.Input(shape=(1,), name='dist_to_dhoby')\n",
    "\n",
    "        self.all_inputs = [month,\n",
    "                           storey_range,\n",
    "                           flat_model_type,\n",
    "                           floor_area_sqm,\n",
    "                           remaining_lease_years,\n",
    "                           degree_centrality,\n",
    "                           eigenvector_centrality,\n",
    "                           dist_to_nearest_stn,\n",
    "                           dist_to_dhoby]\n",
    "\n",
    "        # integer categorical feature modified encoded\n",
    "        month_modified_encoded = self.modified_encode_categorical_feature(month, 'month', self.train_ds, False)\n",
    "\n",
    "        # string categorical features modified encoded\n",
    "        storey_range_modified_encoded = self.modified_encode_categorical_feature(storey_range, 'storey_range',\n",
    "                                                                                 self.train_ds, True)\n",
    "        flat_model_type_modified_encoded = self.modified_encode_categorical_feature(flat_model_type, 'flat_model_type',\n",
    "                                                                                    self.train_ds, True)\n",
    "\n",
    "        # numerical features encoded\n",
    "        floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, 'floor_area_sqm', self.train_ds)\n",
    "        remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, 'remaining_lease_years',\n",
    "                                                                 self.train_ds)\n",
    "        degree_centrality_encoded = encode_numerical_feature(degree_centrality, 'degree_centrality', self.train_ds)\n",
    "        eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, 'eigenvector_centrality',\n",
    "                                                                  self.train_ds)\n",
    "        dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, 'dist_to_nearest_stn',\n",
    "                                                               self.train_ds)\n",
    "        dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, 'dist_to_dhoby', self.train_ds)\n",
    "\n",
    "        # integer categorical feature embedded and flattened\n",
    "        month_embedding = tf.keras.layers.Embedding(input_dim=13, output_dim=(13 - 1) // 2)(month_modified_encoded)\n",
    "        month_embedding_flattened = tf.keras.layers.Flatten()(month_embedding)\n",
    "\n",
    "        # string categorical features embedded and flattened\n",
    "        storey_range_embedding = tf.keras.layers.Embedding(input_dim=18, output_dim=(18 - 1) // 2)(\n",
    "            storey_range_modified_encoded)\n",
    "        storey_range_embedding_flattened = tf.keras.layers.Flatten()(storey_range_embedding)\n",
    "        flat_model_type_embedding = tf.keras.layers.Embedding(input_dim=44, output_dim=(44 - 1) // 2)(\n",
    "            flat_model_type_modified_encoded)\n",
    "        flat_model_type_embedding_flattened = tf.keras.layers.Flatten()(flat_model_type_embedding)\n",
    "\n",
    "        self.all_features = [month_embedding_flattened,\n",
    "                             storey_range_embedding_flattened,\n",
    "                             flat_model_type_embedding_flattened,\n",
    "                             floor_area_sqm_encoded,\n",
    "                             remaining_lease_years_encoded,\n",
    "                             degree_centrality_encoded,\n",
    "                             eigenvector_centrality_encoded,\n",
    "                             dist_to_nearest_stn_encoded,\n",
    "                             dist_to_dhoby_encoded]\n",
    "\n",
    "    def get_all_inputs(self):\n",
    "        return self.all_inputs\n",
    "\n",
    "    def get_all_features(self):\n",
    "        return self.all_features\n",
    "\n",
    "\n",
    "class Question3RFE():\n",
    "\n",
    "    def __init__(self, train_ds, val_ds, all_inputs, all_features, features_mask, epochs=100):\n",
    "        self.all_inputs = all_inputs\n",
    "        self.all_features = all_features\n",
    "        self.features_mask = features_mask\n",
    "        self.epochs = epochs\n",
    "        self.seed = 42\n",
    "        self.filepath = 'rfe/p2q3b-{}.ckpt'.format(features_mask)\n",
    "        self.history = None\n",
    "\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def prepare_model(self):\n",
    "        features_mask_list = list(map(int, list(self.features_mask)))\n",
    "        self.rfe_inputs = []\n",
    "        self.rfe_features = []\n",
    "\n",
    "        for index, value in enumerate(features_mask_list):\n",
    "            if value:\n",
    "                self.rfe_inputs.append(self.all_inputs[index])\n",
    "                self.rfe_features.append(self.all_features[index])\n",
    "\n",
    "        self.rfe_features = tf.keras.layers.concatenate(self.rfe_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def r_square(y_true, y_pred):\n",
    "        SS_res = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))\n",
    "        SS_tot = tf.keras.backend.sum(tf.keras.backend.square(y_true - tf.keras.backend.mean(y_true)))\n",
    "        return (1 - SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def create_model(self):\n",
    "        # create the model\n",
    "        x = tf.keras.layers.Dense(units=10, activation='relu')(self.rfe_features)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='linear')(x)\n",
    "        self.model = tf.keras.Model(inputs=self.rfe_inputs, outputs=output)\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "                           loss=tf.keras.losses.MeanSquaredError(),\n",
    "                           metrics=[self.r_square])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self):\n",
    "        # train the model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.filepath,\n",
    "                                                                       monitor='val_loss',\n",
    "                                                                       verbose=0,\n",
    "                                                                       save_best_only=True,\n",
    "                                                                       save_weights_only=True,\n",
    "                                                                       mode='min')\n",
    "        self.history = self.model.fit(x=train_ds,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=0,\n",
    "                                      callbacks=[EarlyStopping(monitor='val_loss', patience=10),\n",
    "                                                 model_checkpoint_callback],\n",
    "                                      validation_data=self.val_ds)\n",
    "\n",
    "    def get_rfe_result(self):\n",
    "        # get lowest test error epoch and R2 value\n",
    "        lowest_test_error, lowest_test_error_epoch = min(\n",
    "            (value, index + 1) for (index, value) in enumerate(self.history.history['val_loss']))\n",
    "        lowest_test_error_epoch_R2_value = self.history.history['val_r_square'][lowest_test_error_epoch - 1]\n",
    "        return (lowest_test_error, lowest_test_error_epoch_R2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# num_features_key -> features_mask_key -> (test_error, R2)\n",
    "rfe_results = {}\n",
    "rfe_elimination_order = []\n",
    "\n",
    "\n",
    "def question3_run_fre(train_ds, val_ds, all_inputs, all_features, features_mask, epochs=100):\n",
    "    features_mask_list = list(map(int, list(features_mask)))\n",
    "    num_features_key = \"num_features: {}\".format(sum(features_mask_list) - 1)\n",
    "\n",
    "    if num_features_key == \"num_features: 0\":\n",
    "        return\n",
    "\n",
    "    if not rfe_results.get(num_features_key):\n",
    "        rfe_results[num_features_key] = {}\n",
    "\n",
    "    overall_lowest_test_error = None\n",
    "    overall_lowest_test_error_features_mask = None\n",
    "\n",
    "    for index, value in enumerate(features_mask_list):\n",
    "        new_features_mask_list = [_ for _ in features_mask_list]\n",
    "        if value:\n",
    "            new_features_mask_list[index] = 0\n",
    "            new_features_mask = \"\".join(list(map(str, new_features_mask_list)))\n",
    "            features_mask_key = \"features_mask_key: {}\".format(new_features_mask)\n",
    "\n",
    "            question3RFE = Question3RFE(train_ds, val_ds, all_inputs, all_features, new_features_mask, epochs)\n",
    "            question3RFE.prepare_model()\n",
    "            question3RFE.create_model()\n",
    "            question3RFE.train_model()\n",
    "\n",
    "            lowest_test_error, lowest_test_error_epoch_R2_value = question3RFE.get_rfe_result()\n",
    "            rfe_results[num_features_key][features_mask_key] = (lowest_test_error, lowest_test_error_epoch_R2_value)\n",
    "\n",
    "            if not overall_lowest_test_error or overall_lowest_test_error > lowest_test_error:\n",
    "                overall_lowest_test_error = lowest_test_error\n",
    "                overall_lowest_test_error_features_mask = new_features_mask\n",
    "\n",
    "    rfe_elimination_order.append(overall_lowest_test_error_features_mask)\n",
    "\n",
    "    question3_run_fre(train_ds, val_ds, all_inputs, all_features, overall_lowest_test_error_features_mask, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question3RFEHelper = Question3RFEHelper(train_ds, val_ds)\n",
    "question3RFEHelper.prepare_rfe()\n",
    "question3_run_fre(train_ds, val_ds, question3RFEHelper.get_all_inputs(), question3RFEHelper.get_all_features(),\n",
    "                  features_mask=\"111111111\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Compare the performances of the model with all 9 input features (from Q2) and the best model arrived at by RFE, in terms of both RMSE and R2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfe_elimination_order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfe_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for features_mask in rfe_elimination_order:\n",
    "    features_mask_list = list(map(int, list(features_mask)))\n",
    "    num_features_key = \"num_features: {}\".format(sum(features_mask_list))\n",
    "    features_mask_key = \"num_features_mask_key: {}\".format(features_mask)\n",
    "\n",
    "    print(\"{} with test error {}\".format(features_mask_key, rfe_results[num_features_key][features_mask_key]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "d) By examining the changes in model performance whenever a feature is removed, evaluate the usefulness of each feature for the task of HDB resale price prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "start_1b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}