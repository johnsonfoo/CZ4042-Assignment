{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YM8unKH7nSIM"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# sklearn libraries are useful for preprocessing, performance measures, etc.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmTBl3xoqA9g"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An4b9d12p8u2"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./features_30_sec.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHZQbQVzqGIA"
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zp_RDMgqL9Z"
   },
   "source": [
    "Split and scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fNTd9t7qHCI"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['label', 'filename', 'length']\n",
    "\n",
    "\n",
    "def prepare_dataset(df, columns_to_drop, test_size, random_state):\n",
    "    # Encode the labels from 0 to n_classes-1\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "    # devide data to train and test\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # scale the training inputs\n",
    "    x_train = df_train.drop(columns_to_drop, axis=1)\n",
    "    y_train = df_train['label'].to_numpy()\n",
    "\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    x_train_scaled = standard_scaler.fit_transform(x_train)\n",
    "\n",
    "    #scale and prepare testing data\n",
    "    x_test = df_test.drop(columns_to_drop, axis=1)\n",
    "    x_test_scaled = standard_scaler.transform(x_test)\n",
    "    y_test = df_test['label'].to_numpy()\n",
    "\n",
    "    return x_train_scaled, y_train, x_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fNTd9t7qHCI"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_dataset(df, columns_to_drop, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Design a feedforward deep neural network (DNN) which consists of an input layer, one hidden layer of 16 neurons with ReLU activation function, and an output softmax layer. Use an stochastic gradient descent with ‘adam’ optimizer with default parameters, and batch size = 1. Apply dropout of probability 0.3 to the hidden layer. Divide the dataset into a 70:30 ratio for training and testing. Use appropriate scaling of input features. We solely assume that there are only two datasets here: training & test. We would look into validation in Question 2 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Question1():\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, epochs=50, num_hidden_neurons=16, batch_size=1):\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = 0\n",
    "        self.history = None\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def create_model(self, num_hidden_neurons=None):\n",
    "        # create the model\n",
    "        num_hidden_neurons = num_hidden_neurons or self.num_hidden_neurons\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self, batch_size=None):\n",
    "        # train the model\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        self.history = self.model.fit(x=self.X_train, y=self.y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(self.X_test, self.y_test))\n",
    "\n",
    "    def plot_model(self, variable='loss'):\n",
    "        # plot learning curves\n",
    "        plt.plot(self.history.history[variable], label='Model training {}'.format(variable))\n",
    "        plt.plot(self.history.history['val_{}'.format(variable)], label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use the training dataset to train the model for 50 epochs. Note: Use 50 epochs for subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1 = Question1(X_train, y_train, X_test, y_test)\n",
    "question1.create_model()\n",
    "question1.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot accuracies on training and test data against training epochs and\n",
    "comment on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1.plot_model('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Plot the losses on training and test data against training epochs. State the approximate number of epochs where the test error begins to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1.plot_model('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "In this question, we will compare the performance of the model using stochastic gradient descent and mini-batch gradient descent, as well as determining the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 3-fold cross-validation on training partition to perform parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTakenPerEpochCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_begin_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.times.append(time.time() - self.epoch_begin_time)\n",
    "\n",
    "\n",
    "class Question2():\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, epochs=50, num_hidden_neurons=16, batch_size=1,\n",
    "                 num_experiments=10):\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.num_experiments = num_experiments\n",
    "        self.batch_sizes = [1, 4, 8, 16, 32, 64]\n",
    "        self.num_folds = 3\n",
    "        self.seed = 0\n",
    "        self.history = None\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def create_model(self, num_hidden_neurons=None):\n",
    "        # create the model\n",
    "        num_hidden_neurons = num_hidden_neurons or self.num_hidden_neurons\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def cross_validate_run_experiment(self, experiment_X_train, experiment_y_train):\n",
    "        size_fold = len(experiment_X_train) // self.num_folds\n",
    "        # experiment_accuracy maps batch_size_key -> num_fold_key -> epoch_key -> val_accuracy\n",
    "        experiment_accuracy = {}\n",
    "        # experiment_time maps batch_size_key -> num_fold_key -> time_taken_list\n",
    "        experiment_time = {}\n",
    "\n",
    "        for num_fold in range(self.num_folds):\n",
    "            start, end = num_fold * size_fold, (num_fold + 1) * size_fold\n",
    "            cv_X_train = np.append(experiment_X_train[:start], experiment_X_train[end:], axis=0)\n",
    "            cv_y_train = np.append(experiment_y_train[:start], experiment_y_train[end:], axis=0)\n",
    "            cv_X_test, cv_y_test = experiment_X_train[start:end], experiment_y_train[start:end]\n",
    "\n",
    "            for batch_size in self.batch_sizes:\n",
    "\n",
    "                cv_model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(units=self.num_hidden_neurons, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "                    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "                ])\n",
    "\n",
    "                cv_model.compile(optimizer='adam',\n",
    "                                 loss='sparse_categorical_crossentropy',\n",
    "                                 metrics=['accuracy'])\n",
    "\n",
    "                callback = TimeTakenPerEpochCallback()\n",
    "\n",
    "                cv_history = cv_model.fit(x=cv_X_train, y=cv_y_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          epochs=self.epochs,\n",
    "                                          verbose=0,\n",
    "                                          callbacks=[callback],\n",
    "                                          validation_data=(cv_X_test, cv_y_test))\n",
    "\n",
    "                batch_size_key = \"batch_size: {}\".format(batch_size)\n",
    "                num_fold_key = \"num_fold: {}\".format(num_fold)\n",
    "\n",
    "                # populate experiment_result\n",
    "                if not experiment_accuracy.get(batch_size_key):\n",
    "                    experiment_accuracy[batch_size_key] = {}\n",
    "                if not experiment_accuracy[batch_size_key].get(num_fold_key):\n",
    "                    experiment_accuracy[batch_size_key][num_fold_key] = {}\n",
    "\n",
    "                for epoch, val_accuracy in enumerate(cv_history.history['val_accuracy']):\n",
    "                    epoch_key = \"epoch: {}\".format(epoch)\n",
    "                    experiment_accuracy[batch_size_key][num_fold_key][epoch_key] = val_accuracy\n",
    "\n",
    "                # populate experiment_time\n",
    "                if not experiment_time.get(batch_size_key):\n",
    "                    experiment_time[batch_size_key] = {}\n",
    "\n",
    "                experiment_time[batch_size_key][num_fold_key] = callback.times\n",
    "\n",
    "        return (experiment_accuracy, experiment_time)\n",
    "\n",
    "    def cross_validate_model(self):\n",
    "        size_X_train = len(self.X_train)\n",
    "        index = np.arange(size_X_train)\n",
    "        # experiment_accuracies maps experiment_key -> batch_size_key -> num_fold_key -> epoch_key -> val_accuracy\n",
    "        experiment_accuracies = {}\n",
    "        # experiment_times maps experiment_key -> batch_size_key -> num_fold_key -> time_taken_per_epoch_list\n",
    "        experiment_times = {}\n",
    "\n",
    "        for experiment in range(self.num_experiments):\n",
    "            np.random.shuffle(index)\n",
    "            experiment_X_train, experiment_y_train = self.X_train[index], self.y_train[index]\n",
    "            experiment_accuracy, experiment_time = self.cross_validate_run_experiment(experiment_X_train,\n",
    "                                                                                      experiment_y_train)\n",
    "            experiment_key = \"experiment: {}\".format(experiment)\n",
    "            experiment_accuracies[experiment_key] = experiment_accuracy\n",
    "            experiment_times[experiment_key] = experiment_time\n",
    "\n",
    "        experiment_mean_accuracies = self.cross_validate_get_experiment_mean_accuracies(experiment_accuracies)\n",
    "        self.mean_of_experiment_mean_accuracies = self.cross_validate_get_mean_of_experiment_mean_accuracies(\n",
    "            experiment_mean_accuracies)\n",
    "\n",
    "        self.experiment_median_times = self.cross_validate_get_median_of_experiment_times(experiment_times)\n",
    "\n",
    "    def get_mean_of_experiment_mean_accuracies(self):\n",
    "        return self.mean_of_experiment_mean_accuracies\n",
    "\n",
    "    def get_experiment_median_times(self):\n",
    "        return self.experiment_median_times\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_validate_get_experiment_mean_accuracies(experiment_accuracies):\n",
    "        # experiment_mean_accuracies maps experiment_key -> batch_size_key -> epoch -> mean_val_accuracy\n",
    "        experiment_mean_accuracies = {}\n",
    "\n",
    "        for experiment_key in experiment_accuracies.keys():\n",
    "            # experiment_accuracy maps batch_size_key -> num_fold_key -> epoch_key -> val_accuracy\n",
    "            experiment_accuracy = experiment_accuracies[experiment_key]\n",
    "            # experiment_mean_accuracy maps batch_size_key -> epoch_key -> mean_val_accuracy\n",
    "            experiment_mean_accuracy = {}\n",
    "\n",
    "            for batch_size_key in experiment_accuracy.keys():\n",
    "                if not experiment_mean_accuracy.get(batch_size_key):\n",
    "                    experiment_mean_accuracy[batch_size_key] = {}\n",
    "                for num_fold_key in experiment_accuracy[batch_size_key].keys():\n",
    "                    for epoch_key, val_accuracy in experiment_accuracy[batch_size_key][num_fold_key].items():\n",
    "                        if not experiment_mean_accuracy[batch_size_key].get(epoch_key):\n",
    "                            experiment_mean_accuracy[batch_size_key][epoch_key] = []\n",
    "                        experiment_mean_accuracy[batch_size_key][epoch_key].append(val_accuracy)\n",
    "\n",
    "            for batch_size_key in experiment_mean_accuracy.keys():\n",
    "                for epoch_key in experiment_mean_accuracy[batch_size_key].keys():\n",
    "                    experiment_mean_accuracy[batch_size_key][epoch_key] = np.mean(\n",
    "                        experiment_mean_accuracy[batch_size_key][epoch_key])\n",
    "\n",
    "            experiment_mean_accuracies[experiment_key] = experiment_mean_accuracy\n",
    "\n",
    "        return experiment_mean_accuracies\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_validate_get_mean_of_experiment_mean_accuracies(experiment_mean_accuracies):\n",
    "        # mean_of_experiment_mean_accuracies maps batch_size_key -> epoch -> mean_of_mean_val_accuracy\n",
    "        mean_of_experiment_mean_accuracies = {}\n",
    "\n",
    "        for experiment_key in experiment_mean_accuracies.keys():\n",
    "            # experiment_mean_accuracy maps batch_size_key -> epoch_key -> mean_val_accuracy\n",
    "            experiment_mean_accuracy = experiment_mean_accuracies[experiment_key]\n",
    "\n",
    "            for batch_size_key in experiment_mean_accuracy.keys():\n",
    "                if not mean_of_experiment_mean_accuracies.get(batch_size_key):\n",
    "                    mean_of_experiment_mean_accuracies[batch_size_key] = {}\n",
    "                for epoch_key, mean_val_accuracy in experiment_mean_accuracy[batch_size_key].items():\n",
    "                    if not mean_of_experiment_mean_accuracies[batch_size_key].get(epoch_key):\n",
    "                        mean_of_experiment_mean_accuracies[batch_size_key][epoch_key] = []\n",
    "                    mean_of_experiment_mean_accuracies[batch_size_key][epoch_key].append(mean_val_accuracy)\n",
    "\n",
    "        for batch_size_key in mean_of_experiment_mean_accuracies.keys():\n",
    "            for epoch_key in experiment_mean_accuracy[batch_size_key].keys():\n",
    "                mean_of_experiment_mean_accuracies[batch_size_key][epoch_key] = np.mean(\n",
    "                    mean_of_experiment_mean_accuracies[batch_size_key][epoch_key])\n",
    "\n",
    "        return mean_of_experiment_mean_accuracies\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_validate_get_median_of_experiment_times(experiment_times):\n",
    "        # experiment_median_times maps batch_size_key -> median_time_taken_per_epoch\n",
    "        experiment_median_times = {}\n",
    "\n",
    "        for experiment_key in experiment_times.keys():\n",
    "            # experiment_time maps batch_size_key -> num_fold_key -> time_taken_per_epoch_list\n",
    "            experiment_time = experiment_times[experiment_key]\n",
    "\n",
    "            for batch_size_key in experiment_time.keys():\n",
    "                for num_fold_key in experiment_time[batch_size_key]:\n",
    "                    if not experiment_median_times.get(batch_size_key):\n",
    "                        experiment_median_times[batch_size_key] = []\n",
    "\n",
    "                    experiment_median_times[batch_size_key].extend(experiment_time[batch_size_key][num_fold_key])\n",
    "\n",
    "        for batch_size_key in experiment_median_times.keys():\n",
    "            experiment_median_times[batch_size_key] = np.median(\n",
    "                experiment_median_times[batch_size_key])\n",
    "\n",
    "        return experiment_median_times\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self, batch_size=None):\n",
    "        # train the model\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        self.history = self.model.fit(x=self.X_train, y=self.y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(self.X_test, self.y_test))\n",
    "\n",
    "    def plot_model(self, variable='loss', epoch_start=1, epoch_end=None):\n",
    "        # plot learning curves\n",
    "        train_epochs = self.history.history[variable][epoch_start - 1:epoch_end]\n",
    "        val_epochs = self.history.history['val_{}'.format(variable)][epoch_start - 1:epoch_end]\n",
    "        number_epochs = len(self.history.history[variable][epoch_start - 1:epoch_end])\n",
    "\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), train_epochs,\n",
    "                 label='Model training {}'.format(variable))\n",
    "        plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def cross_validate_plot_results(self, epoch_start=1, epoch_end=None):\n",
    "        for batch_size_key in mean_of_experiment_mean_accuracies.keys():\n",
    "            val_epochs = [mean_of_experiment_mean_accuracies[batch_size_key]['epoch: {}'.format(epoch)] for epoch in\n",
    "                          range(self.epochs)]\n",
    "            number_epochs = len(val_epochs[epoch_start - 1:epoch_end])\n",
    "            plt.plot(range(epoch_start, epoch_start + number_epochs), val_epochs, label=batch_size_key)\n",
    "\n",
    "        plt.title('Model mean cross-validation accuracy')\n",
    "        plt.ylabel('mean cross-validation accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot mean cross-validation accuracies over the training epochs for different batch sizes. Limit search space to batch sizes {1,4,8,16,32,64}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question2 = Question2(X_train, y_train, X_test, y_test, epochs=20, num_experiments=2)\n",
    "question2.cross_validate_model()\n",
    "mean_of_experiment_mean_accuracies = question2.get_mean_of_experiment_mean_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2.cross_validate_plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(\"{}\\t{}\".format(k, v[\"epoch: {}\".format(question2.epochs - 1)]) for k, v in\n",
    "                mean_of_experiment_mean_accuracies.items()))\n",
    "\n",
    "optimal_batch_size = None\n",
    "optimal_batch_size_experiment_value = None\n",
    "\n",
    "for batch_size_key in mean_of_experiment_mean_accuracies.keys():\n",
    "    epoch_key = \"epoch: {}\".format(question2.epochs - 1)\n",
    "    if not optimal_batch_size_experiment_value or mean_of_experiment_mean_accuracies[batch_size_key][\n",
    "        epoch_key] > optimal_batch_size_experiment_value:\n",
    "        optimal_batch_size_experiment_value = mean_of_experiment_mean_accuracies[batch_size_key][epoch_key]\n",
    "        optimal_batch_size = int(batch_size_key.split(\": \")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create a table of median time taken to train the network for one epoch against different batch sizes. (Hint: Introduce a callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_median_times = question2.get_experiment_median_times()\n",
    "print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in experiment_median_times.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Select the optimal batch size and state reasons for your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the difference between mini-batch gradient descent and stochastic gradient descent and what does this mean for model training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the train and test accuracies against epochs for the optimal batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question2.create_model()\n",
    "question2.train_model(batch_size=optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2.plot_model('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Find the optimal number of hidden neurons for the 2-layer network (i.e., one hidden layer) designed in Question 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question3():\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, epochs=50, num_hidden_neurons=16, batch_size=1,\n",
    "                 num_experiments=10):\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.num_experiments = num_experiments\n",
    "        self.num_hidden_neurons_list = [8, 16, 32, 64]\n",
    "        self.num_folds = 3\n",
    "        self.seed = 0\n",
    "        self.history = None\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def create_model(self, num_hidden_neurons=None):\n",
    "        # create the model\n",
    "        num_hidden_neurons = num_hidden_neurons or self.num_hidden_neurons\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def cross_validate_run_experiment(self, experiment_X_train, experiment_y_train):\n",
    "        size_fold = len(experiment_X_train) // self.num_folds\n",
    "        # num_hidden_neurons_key -> num_fold_key -> val_accuracy\n",
    "        experiment_accuracy = {}\n",
    "\n",
    "        for num_fold in range(self.num_folds):\n",
    "            start, end = num_fold * size_fold, (num_fold + 1) * size_fold\n",
    "            cv_X_train = np.append(experiment_X_train[:start], experiment_X_train[end:], axis=0)\n",
    "            cv_y_train = np.append(experiment_y_train[:start], experiment_y_train[end:], axis=0)\n",
    "            cv_X_test, cv_y_test = experiment_X_train[start:end], experiment_y_train[start:end]\n",
    "\n",
    "            for num_hidden_neurons in self.num_hidden_neurons_list:\n",
    "\n",
    "                cv_model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "                    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "                ])\n",
    "\n",
    "                cv_model.compile(optimizer='adam',\n",
    "                                 loss='sparse_categorical_crossentropy',\n",
    "                                 metrics=['accuracy'])\n",
    "\n",
    "                cv_history = cv_model.fit(x=cv_X_train, y=cv_y_train,\n",
    "                                          batch_size=self.batch_size,\n",
    "                                          epochs=self.epochs,\n",
    "                                          verbose=0,\n",
    "                                          validation_data=(cv_X_test, cv_y_test))\n",
    "\n",
    "                num_hidden_neurons_key = \"num_hidden_neurons: {}\".format(num_hidden_neurons)\n",
    "                num_fold_key = \"num_fold: {}\".format(num_fold)\n",
    "\n",
    "                # populate experiment_result\n",
    "                if not experiment_accuracy.get(num_hidden_neurons_key):\n",
    "                    experiment_accuracy[num_hidden_neurons_key] = {}\n",
    "\n",
    "                experiment_accuracy[num_hidden_neurons_key][num_fold_key] = cv_history.history['val_accuracy'][\n",
    "                    self.epochs - 1]\n",
    "\n",
    "        return experiment_accuracy\n",
    "\n",
    "    def cross_validate_model(self):\n",
    "        size_X_train = len(self.X_train)\n",
    "        index = np.arange(size_X_train)\n",
    "        # experiment_key -> num_hidden_neurons_key -> num_fold_key -> val_accuracy\n",
    "        experiment_accuracies = {}\n",
    "\n",
    "        for experiment in range(self.num_experiments):\n",
    "            np.random.shuffle(index)\n",
    "            experiment_X_train, experiment_y_train = self.X_train[index], self.y_train[index]\n",
    "            experiment_accuracy = self.cross_validate_run_experiment(experiment_X_train, experiment_y_train)\n",
    "            experiment_key = \"experiment: {}\".format(experiment)\n",
    "            experiment_accuracies[experiment_key] = experiment_accuracy\n",
    "\n",
    "        experiment_mean_accuracies = self.cross_validate_get_experiment_mean_accuracies(experiment_accuracies)\n",
    "        mean_of_experiment_mean_accuracies = self.cross_validate_get_mean_of_experiment_mean_accuracies(\n",
    "            experiment_mean_accuracies)\n",
    "\n",
    "        return mean_of_experiment_mean_accuracies\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_validate_get_experiment_mean_accuracies(experiment_accuracies):\n",
    "        # experiment_key -> num_hidden_neurons_key -> mean_val_accuracy\n",
    "        experiment_mean_accuracies = {}\n",
    "\n",
    "        for experiment_key in experiment_accuracies.keys():\n",
    "            experiment_accuracy = experiment_accuracies[experiment_key]\n",
    "            experiment_mean_accuracy = {}\n",
    "\n",
    "            for num_hidden_neurons_key in experiment_accuracy.keys():\n",
    "                experiment_mean_accuracy[num_hidden_neurons_key] = np.mean(\n",
    "                    list(experiment_accuracy[num_hidden_neurons_key].values()))\n",
    "\n",
    "            experiment_mean_accuracies[experiment_key] = experiment_mean_accuracy\n",
    "\n",
    "        return experiment_mean_accuracies\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_validate_get_mean_of_experiment_mean_accuracies(experiment_mean_accuracies):\n",
    "        # num_hidden_neurons_key -> mean_of_mean_val_accuracy\n",
    "        mean_of_experiment_mean_accuracies = {}\n",
    "\n",
    "        for experiment_key in experiment_mean_accuracies.keys():\n",
    "            experiment_mean_accuracy = experiment_mean_accuracies[experiment_key]\n",
    "\n",
    "            for num_hidden_neurons_key in experiment_mean_accuracy:\n",
    "                if not mean_of_experiment_mean_accuracies.get(num_hidden_neurons_key):\n",
    "                    mean_of_experiment_mean_accuracies[num_hidden_neurons_key] = []\n",
    "                mean_of_experiment_mean_accuracies[num_hidden_neurons_key].append(\n",
    "                    experiment_mean_accuracies[experiment_key][num_hidden_neurons_key])\n",
    "\n",
    "        for num_hidden_neurons_key in mean_of_experiment_mean_accuracies.keys():\n",
    "            mean_of_experiment_mean_accuracies[num_hidden_neurons_key] = np.mean(\n",
    "                mean_of_experiment_mean_accuracies[num_hidden_neurons_key])\n",
    "\n",
    "        return mean_of_experiment_mean_accuracies\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self, batch_size=None):\n",
    "        # train the model\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        self.history = self.model.fit(x=self.X_train, y=self.y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(self.X_test, self.y_test))\n",
    "\n",
    "    def plot_model(self, variable='loss'):\n",
    "        # plot learning curves\n",
    "        plt.plot(self.history.history[variable], label='Model training {}'.format(variable))\n",
    "        plt.plot(self.history.history['val_{}'.format(variable)], label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the cross-validation accuracies against training epochs for different numbers of hidden-layer neurons. Limit the search space of the number of neurons to {8, 16, 32, 64}. Continue using 3-fold cross validation on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question3 = Question3(X_train, y_train, X_test, y_test, batch_size=optimal_batch_size)\n",
    "mean_of_experiment_mean_accuracies = question3.cross_validate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_of_experiment_mean_accuracies)\n",
    "\n",
    "best_mean_of_experiment_mean_accuracies = max(mean_of_experiment_mean_accuracies,\n",
    "                                              key=mean_of_experiment_mean_accuracies.get)\n",
    "print(best_mean_of_experiment_mean_accuracies)\n",
    "\n",
    "optimal_num_hidden_neurons = int(best_mean_of_experiment_mean_accuracies.split(\": \")[-1])\n",
    "print(optimal_num_hidden_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Select the optimal number of neurons for the hidden layer. State the rationale for your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Plot the train and test accuracies against training epochs with the optimal number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3.create_model(num_hidden_neurons=optimal_num_hidden_neurons)\n",
    "question3.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3.plot_model('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What other parameters could possibly be tuned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "After you are done with the 2-layer network, design a 3-layer network with two hiddenlayers with ReLU activation, each consisting of the optimal number of neurons you obtained in Question 3, (apply a dropout with a probability of 0.3 for each hidden layer), and train it with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question4():\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, epochs=50, num_hidden_neurons=16, batch_size=1):\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = 0\n",
    "        self.history = None\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def create_model(self, num_hidden_neurons=None):\n",
    "        # create the model\n",
    "        num_hidden_neurons = num_hidden_neurons or self.num_hidden_neurons\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "            tf.keras.layers.Dense(units=num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(rate=0.3, seed=self.seed),\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self, batch_size=None):\n",
    "        # train the model\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        self.history = self.model.fit(x=self.X_train, y=self.y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(self.X_test, self.y_test))\n",
    "\n",
    "    def plot_model(self, variable='loss'):\n",
    "        # plot learning curves\n",
    "        plt.plot(self.history.history[variable], label='Model training {}'.format(variable))\n",
    "        plt.plot(self.history.history['val_{}'.format(variable)], label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the train and test accuracy of the 3-layer network against training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_num_hidden_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question4 = Question4(X_train, y_train, X_test, y_test, num_hidden_neurons=optimal_num_hidden_neurons)\n",
    "question4.create_model()\n",
    "question4.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question4.plot_model('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compare and comment on the performances of the optimal 2-layer network from your hyperparameter tuning in Question 2 and 3 and the 3-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (let’s dig deeper!)\n",
    "We are going to dissect the purpose of dropout in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Question5():\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, epochs=50, num_hidden_neurons=16, batch_size=1):\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = 0\n",
    "        self.history = None\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "\n",
    "    def create_model(self):\n",
    "        # create the model\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.num_hidden_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def summarize_model(self):\n",
    "        # summarize the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train_model(self, batch_size=None):\n",
    "        # train the model\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        self.history = self.model.fit(x=self.X_train, y=self.y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(self.X_test, self.y_test))\n",
    "\n",
    "    def plot_model(self, variable='loss'):\n",
    "        # plot learning curves\n",
    "        plt.plot(self.history.history[variable], label='Model training {}'.format(variable))\n",
    "        plt.plot(self.history.history['val_{}'.format(variable)], label='Model testing {}'.format(variable))\n",
    "        plt.title('Model {}'.format(variable))\n",
    "        plt.ylabel('{}'.format(variable))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Why do we add dropouts? Investigate the purpose of dropouts by removing dropouts from your original 2-layer network (before changing the batch size and number of neurons). Plot accuracies on training and test data with neural network without dropout. Plot as well the losses on training and test data with neural network without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question5 = Question5(X_train, y_train, X_test, y_test)\n",
    "question5.create_model()\n",
    "question5.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question5.plot_model('accuracy')\n",
    "question5.plot_model('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Explain the effect of removing dropouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is another approach that you could take to address overfitting in the model?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBI19y+/jqNpPI0QeiLTYf",
   "collapsed_sections": [],
   "name": "StarterCode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}